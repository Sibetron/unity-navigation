{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project we will train an agent to navigate a large square world and collect yellow bananas.\n",
    "\n",
    "A reward of +1 is provided for collecting a yellow banana, and a reward of -1 is provided for collecting a blue banana.  Thus, the goal of your agent is to collect as many yellow bananas as possible while avoiding blue bananas.  \n",
    "\n",
    "The state space has 37 dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  Given this information, the agent has to learn how to best select actions.  Four discrete actions are available, corresponding to:\n",
    "- **`0`** - move forward.\n",
    "- **`1`** - move backward.\n",
    "- **`2`** - turn left.\n",
    "- **`3`** - turn right.\n",
    "\n",
    "The task is episodic, and in order to solve the environment, the agent must get an average score of +13 over 100 consecutive episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using vanilla Deep Q-Learning (DQN) algorithm to solve this project.\n",
    "\n",
    "The algorithm uses images one screen at a time and it produces a vector of action values with the max value indicating the action to take.\n",
    "As a reinforcement signal we feed back the change in score at each time step. When we initialize the neural network with random values the actions taken are all over the place. Overtime the network associates situations and sequences in the game with appropriate actions and learn to play well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for network architecture we use a simple neural network with 3 linear layers and relu activation function. The first linear layer takes dimension of (37,64), second takes (64,64) and last layer takes dimension (64,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "self.fc1 = nn.Linear(state_size, fc1_units)<br/>\n",
    "self.fc2 = nn.Linear(fc1_units, fc2_units)<br/>\n",
    "self.fc3 = nn.Linear(fc2_units, action_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src="dqn.png/">\n",
    "<center>source:https://classroom.udacity.com/nanodegrees/nd893</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps shown in the above algorithm are implemented in dqn_agent.py file\n",
    "In the Constructor we initialize the replay buffer and initialize the target and local neural network. In the Step function we take a step and store it in the replay buffer. For every 4 steps we update the target network weights with the current weights. The act function returns actions for the given state as per current policy. In the learn function we update the value of neural network paramenters for batch of experiences from the replay buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent takes the following parameters: <br/>\n",
    "BUFFER_SIZE = int(1e5)     &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;         # replay buffer size <br/>\n",
    "BATCH_SIZE = 64            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  &nbsp;&nbsp;&nbsp;          # minibatch size <br/>\n",
    "GAMMA = 0.99                &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;              # discount factor <br/>\n",
    "TAU = 1e-3      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;          # for soft update of target parameters <br/>\n",
    "LR = 5e-4       &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;         # learning rate <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the parameters for training, <br/>\n",
    "Maximum Training Episodes: 2000 <br/>\n",
    "Maximum Timesteps per Episode: 1000  <br/>\n",
    "Starting Value of Epsilon for Epsilon Greety Action: 1.0  <br/>\n",
    "Minimum Value of Epsilon: 0.01 <br/>\n",
    "Aging Factor for Epsilon: 0.995 <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src="train.png/">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model was able to reach average score of 15 in <500 episodes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src="plot.png/">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prioritized experience replay: Currently we selected experience tuples randomly. A prioritized experienced replay selects experiences based on a priority value. By using prioritized replay we increase the probility of choosing important experience tuples from the sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Using Raninbow DQN: from the following [paper](https://arxiv.org/pdf/1710.02298.pdf) we can see how rainbow DQN was able to outperform other algorithms by combining the best strategies implemented by all other DQN. I believe we might be albe to see performance increase by testing out rainbow DQN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
